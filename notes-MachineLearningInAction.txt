notes-MachineLearningInAction      by xuchangtiao

机器学习的真实含义：
    机器学习能让我们自数据集中受到启发，我们利用计算机来彰显数据背后的真实含义。
    机器学习就是把无序的数据转换成有用的信息。
机器学习的主要任务：
    1.监督学习：
        1）分类。将实例数据划分到合适的分类中。
        2）回归。主要用于预测数值型数据。
    2.无监督学习：
        1）聚类。
        2）密度估计。
训练集：用于训练机器学习算法的数据样本集合。
标称型（离散的）、数值型（连续的）。
开发机器学习应用程序的步骤：
    1.收集数据。利用各种方法收集样本数据。
    2.准备输入数据。确保数据格式符合要求，本书使用Python的List。
    3.分析输入数据。主要确保数据集中没有垃圾数据。
    4.训练算法。无监督学习跳过这一步。
    5.测试算法。不满意则跳回第4步甚至第1步继续。
    6.使用算法。
Python：
    1.优点：
        1）语法清晰，被称为可执行伪代码。
        2）易于操作纯文本文件，易于处理非数值型数据。
        3）使用广泛，存在大量的开发文档。
    2.缺点：
        1）性能问题。

第2章 K-近邻算法
k-近邻算法（kNN）：
    1）简单地说，k-近邻算法采用测量不同特征值之间的距离方法进行分类。
    2）优点：精度高、对异常值不敏感、无数据输入假设。
    3）缺点：计算复杂度高、空间复杂度高。
    4）适用数据范围：数值型和标称型。
    5）工作原理：存在一个样本数据集合，也称为样本训练集，并且样本集中每个数据都存在标签，即我们知道样本集中每一数据与所属分类的对应关系。输入没有标签的新数据后，将新数据的每个特征与样本集中数据对应的特征进行比较，然后算法提取样本集中特征最相似数据（最近邻）的k（通常k不大于20）个分类标签。选择这k个里出现次数最多的分类，作为新数据的分类。

第3章 决策树
决策树：
    1.主要优势：决策树的主要优势就在于数据形式非常容易理解。
    2.优点：计算复杂度不高，输出结果易于理解，对中间值缺失不敏感，可以处理不相关特征数据，还可以持久化地将分类器保存在硬盘上。
    3.缺点：可能会产生过度匹配问题。
    4.适用数据类型：数值型和标称型。
    5.ID3算法可以用于划分标称型数据集。
    6.利用递归的方法将数据集转化为决策树。
    7.使用Python语言内嵌的数据结构字典存储树节点信息。
信息增益：
    1.划分数据集的大原则是：将无序的数据变得更加有序。
    2.获得信息增益最高的特征就是最好的选择。
Python中的pickle序列化对象：
    序列化对象可以在磁盘上保存对象，并在需要的时候读取出来。任何对象都可以执行序列化操作。

第4章 朴素贝叶斯
朴素贝叶斯
    1）优点：在数据较少的情况下仍然有效，可以处理多类别问题。
    2）缺点：对于输入数据的准备方式较为敏感。
    3）使用数据类型：标称型数据。
贝叶斯决策理论的核心思想：选择具有最高概率的决策。
贝叶斯准则：告诉我们如何交换条件概率中的条件与结果。
    如果已知P(x|c)，要求P(c|x)：
        P(c|x) = (P(x|c) * P(c)) / P(x
朴素贝叶斯是贝叶斯分类器的一个扩展，是用于文档分类的常用算法。
朴素贝叶斯的假设：
    1）每个特征之间相互独立。
    2）每个特征同等重要。
为降低概率值为0导致最后乘积也为0的影响，可以将所有次的出现数初始化为1，并将分母初始化为2；
为处理下溢出的问题，可以采用自然对数进行处理，不会有任何损失，且不影响最终结果。
词集模型：在文档中出现的词只记录一次。
词袋模型：在文档中出现的词记录出现的次数。
留存交叉验证：随机选择数据的一部分作为训练集，而剩余部分作为测试集的过程。

第5章 Logistic回归
回归：假设现在有一些数据点，我们用一条直线对这些点进行拟合（该线称为最佳拟合直线），这个拟合过程就称为回归。
Logistic回归：
    1）优点：计算代价不高，易于理解和实现。
    2）缺点：容易欠拟合，分类精度可能不高。
    3）适用数据类型：数值型和标称型数据。
为了实现Logistic回归分类器，我们可以在每个特征上都乘以一个回归系数，然后把所有的结果值相加，将这个综合代入Sigmoid函数中，进而得到一个范围在0-1之间的数值。任何大于0.5的数据被分入1类，小于0.5即被归入0类。Logistic回归也可以被看成是一种概率估计。
梯度上升法：
    基于的思想：要找到某函数的最大值，最好的方法是沿着该函数的梯度方向探寻。
梯度上升算法用来求函数的最大值，而梯度下降算法用来求函数的最小值。
随机梯度上升法：
    梯度上升算法每次更新回归系数时都需要遍历曾哥数据集，计算复杂度高。随机梯度算法是对梯度上升算法的改进，一次仅用一个样本点来更新回归系数。
Logistic回归的目的是寻找一个非线性函数Sigmoid的最佳拟合参数，求解过程可以由最优化算法来完成。在最优化算法中，最常用的就是梯度上升算法，而梯度上升算法又可以简化为随机梯度上升算法。
随机梯度上升算法与梯度上升算法的效果相当，但占用更少的计算资源。此外，随机梯度上升是一个在线算法，它可以在新数据到来时就玩成参数更新，而不需要重新读取整个数据集来进行批处理运算。
机器学习的一个重要问题就是如何处理缺失数据。这个问题没有标准答案，取决于实际应用中的需求。现有一些解决方案，每种方案都各有优缺点。

第6章 支持向量机
支持向量机：
    1）优点：泛化错误率低，计算开销不大，结果易解释。
    2）缺点：对参数调节和核函数的选择敏感，原始分类器不加修改仅适用于处理二类问题。
    3）适用数据类型：数值型和标称型数据。